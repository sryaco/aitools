{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3377,"status":"ok","timestamp":1706549235600,"user":{"displayName":"SONIA YACO","userId":"08079516613956666079"},"user_tz":300},"id":"XfPwzMO0elEX","outputId":"9420b8e8-e6ef-47ed-c342-c64e571b5cc0"},"outputs":[],"source":["#Sonia Yaco\n","#Rutgers University\n","#2024\n","#Produces numeric scores and visual graph of sentiment by paragraph. Output to screen and text and png file.\n","\n","#Load the Drive helper and mount\n","from google.colab import drive#\n","drive.mount('/content/drive/', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ykWbHx35e4Km"},"outputs":[],"source":["#set up paths\n","text_path = \"data\"\n","# Define the path to your text files\n","input_file = text_path + \"/griffis_diary_by_day.txt\"\n","output_file = text_path + \"/out/sentiment_scores.txt\"\n","output_image = text_path + \"/out/sentiment.png\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30688,"status":"ok","timestamp":1706549266273,"user":{"displayName":"SONIA YACO","userId":"08079516613956666079"},"user_tz":300},"id":"_lJV-OuKftwT","outputId":"c9b7aa2d-53de-47b3-8f71-fbac670fa7fc"},"outputs":[],"source":["# load libraries for text analysis\n","from glob import glob\n","import os\n","\n","# clustering and dimension reduction\n","from sklearn.cluster import KMeans\n","from sklearn.decomposition import PCA\n","\n","# for everything else\n","import numpy as np\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","from random import randint\n","import pandas as pd\n","import pickle\n","\n","#for natural language processing\n","import nltk\n","nltk.download('vader_lexicon')\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","nltk.download('punkt')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download(\"stopwords\")\n","from nltk.tokenize import word_tokenize\n","from nltk.chunk import ne_chunk\n","from nltk.util import ngrams\n","from nltk.corpus import stopwords\n","\n","import string\n","from collections import Counter\n","\n","# Load a spaCy model\n","!pip -q install spacy\n","!python -m spacy download en_core_web_sm\n","import spacy\n","from collections import defaultdict\n","nlp = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EjRALQxdHo2n"},"outputs":[],"source":["# remove stop words\n","def remove_stopwords(text):\n","    stop_words = set(stopwords.words(\"english\"))\n","    words = word_tokenize(text)\n","    filtered_words = [word for word in words if word.lower() not in stop_words]\n","    return \" \".join(filtered_words)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2149,"status":"ok","timestamp":1706549268415,"user":{"displayName":"SONIA YACO","userId":"08079516613956666079"},"user_tz":300},"id":"6OnebHDMkpiF","outputId":"8bfc5487-e3f8-46fb-9dda-d48dabe48a99"},"outputs":[],"source":["#sentiment analysis of text file\n","def analyze_sentiment(counter,nscore,text):\n","    #def analyze_sentiment(text):\n","    sia = SentimentIntensityAnalyzer()\n","    sentiment_scores = sia.polarity_scores(text)\n","    nscore = sentiment_scores['compound']\n","\n","    if False:\n","      if sentiment_scores['compound'] > 0.05:\n","        return ( counter, nscore, 'Positive')\n","      elif sentiment_scores['compound'] < -0.05:\n","        return ( counter, nscore, 'Negative' )\n","    else:\n","          #return ( counter, nscore, 'Neutral' )\n","          return ( counter, nscore*10)\n","\n","# Initialize empty lists to store the text file and sentiment of each line\n","lines = []\n","tsentiments = []\n","\n","#lines = \" \"\n","nlines = 1200 # arbitrary guess of file length\n","nsentiment = 0\n","# Open the file in 'r' mode (read mode)\n","with open(input_file , 'r') as file:\n","    # Read the specified number of lines\n","    for i in range(nlines):\n","        line = file.readline()\n","        #tsentiments.append(analyze_sentiment(line,nsentiment))\n","       #print(analyze_sentiment(line,nsentiment))\n","        if not line:\n","            break  # Exit the loop if we reach the end of the file\n","        lines.append(line.strip())  # Remove leading/trailing whitespace and add the line to the list\n","        tsentiments.append(analyze_sentiment(i,nsentiment,line))\n","\n","        #for charting purposes, ignoring text label, only using score\n","\n","# Open the file in write mode and write the list elements\n","with open(output_file, \"w\") as file:\n","    for item in tsentiments:\n","        file.write(str(item) + \"\\n\")\n","\n","print(f\"List saved to {output_file}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":490},"executionInfo":{"elapsed":678,"status":"ok","timestamp":1706550130815,"user":{"displayName":"SONIA YACO","userId":"08079516613956666079"},"user_tz":300},"id":"gH81aR3H3I5W","outputId":"d1244a07-2a53-41fd-be99-5bf1a9e3e611"},"outputs":[],"source":["#map sentiments\n","# Unzip the data into separate x and y lists\n","x_values, y_values = zip(*tsentiments)\n","\n","# Create a scatter plot\n","plt.scatter(x_values, y_values)\n","\n","# Set labels for the axes\n","# Adding labels and title\n","plt.xlabel('Day')\n","plt.ylabel('Sentiment')\n","# Set a title for the plot\n","plt.title('Scatter Plot of Sentiment by Day')\n","\n","# Save to a file\n","plt.savefig(output_image)\n","\n","print(f\"Graph saved to {output_image}\")\n","# Show the plot\n","plt.show()"]}],"metadata":{"colab":{"gpuType":"T4","provenance":[{"file_id":"1QCAfdVVzHVXquO6AQV3GBrBdq_9kWQ4D","timestamp":1698851521368}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
