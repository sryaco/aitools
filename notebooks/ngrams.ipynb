{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#Builds three lists of n-grams from first input file (diary), second input file (biography), and common to both. Defaults to n-grams length of 1"],"metadata":{"id":"RKMYqCWq2TX7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XfPwzMO0elEX","executionInfo":{"status":"ok","timestamp":1706194247303,"user_tz":300,"elapsed":23767,"user":{"displayName":"SONIA YACO","userId":"08079516613956666079"}},"outputId":"9d552a81-2e72-4087-9621-b7a6195bfe09"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["#Load the Drive helper and mount\n","from google.colab import drive#\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive/', force_remount=True)\n"]},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"7EBmeeeW6hsC"}},{"cell_type":"code","source":["#set up paths\n","\n","text_path = \"/content/drive/MyDrive/Colab Notebooks/AIGriffis/data\"\n","#input_file = text_path + \"/first_input.txt\"\n","#input_file2 = text_path + \"/second_input.txt\"\n","input_file = text_path + \"/marg_bio_paragrapheduft8.txt\"\n","input_file2 = text_path + \"/griffis_diary_by_day.txt\"\n","output_file = text_path + \"/out/firstfile_ngrams.txt\"\n","output_file2 = text_path + \"/out/secondfile_ngrams.txt\"\n","output_file3 = text_path + \"/out/common_ngrams.txt\""],"metadata":{"id":"ykWbHx35e4Km"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#for natural language processing\n","import nltk\n","nltk.download('vader_lexicon')\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","nltk.download('punkt')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download(\"stopwords\")\n","from nltk.tokenize import word_tokenize\n","from nltk.chunk import ne_chunk\n","from nltk.util import ngrams\n","from nltk.corpus import stopwords\n","\n","import string\n","from collections import Counter\n","#!pip install networkx\n","import networkx as nx\n","\n","# Load a spaCy model\n","!pip -q install spacy\n","!python -m spacy download en_core_web_sm\n","import spacy\n","from collections import defaultdict\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","import networkx as nx\n","import matplotlib.pyplot as plt\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kjoxFXwe1YLK","executionInfo":{"status":"ok","timestamp":1706194291068,"user_tz":300,"elapsed":43770,"user":{"displayName":"SONIA YACO","userId":"08079516613956666079"}},"outputId":"757eaf84-2eed-40f4-d8f9-abc19a2e7ceb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"stream","name":"stdout","text":["2024-01-25 14:51:04.864500: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-01-25 14:51:04.864581: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-01-25 14:51:04.866495: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-01-25 14:51:04.877364: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-01-25 14:51:06.596493: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Collecting en-core-web-sm==3.6.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.11.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.14)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n","Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.11.17)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.4)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}]},{"cell_type":"code","source":["# Function to clean and tokenize text\n","def clean_and_tokenize(text):\n","    # Tokenize the text\n","    tokens = word_tokenize(text)\n","\n","    # Remove punctuation\n","    tokens = [token.lower() for token in tokens if token not in string.punctuation]\n","    return tokens\n","\n","# Function to generate n-grams from a list of tokens\n","def generate_ngrams(tokens, n):\n","    n_grams = list(ngrams(tokens, n))\n","    return n_grams"],"metadata":{"id":"NvDbFhMFqoI6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# remove stop words\n","def remove_stopwords(text):\n","    stop_words = set(stopwords.words(\"english\"))\n","    words = word_tokenize(text)\n","    filtered_words = [word for word in words if word.lower() not in stop_words]\n","    return \" \".join(filtered_words)"],"metadata":{"id":"HoJR_vjmzXDa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ngram comparison\n","# Function to clean and tokenize text\n","def clean_and_tokenize(text):\n","    # Tokenize the text\n","    tokens = word_tokenize(text)\n","\n","    # Remove punctuation\n","    tokens = [token.lower() for token in tokens if token not in string.punctuation]\n","    return tokens\n","\n","# Function to generate n-grams from a list of tokens\n","def generate_ngrams(tokens, n):\n","    n_grams = list(ngrams(tokens, n))\n","    return n_grams\n","\n","# Function to compare and identify common n-grams\n","\n","\n","def compare_ngrams(text1, text2, n):\n","    tokens1 = clean_and_tokenize(text1)\n","    tokens2 = clean_and_tokenize(text2)\n","\n","    ngrams1 = generate_ngrams(tokens1, n)\n","    ngrams2 = generate_ngrams(tokens2, n)\n","\n","    common_ngrams = set(ngrams1).intersection(ngrams2)\n","\n","    return common_ngrams\n","\n","# Read the contents of the two text files\n","\n","with open(input_file, \"r\", encoding=\"utf-8\") as file1, open(input_file2, \"r\", encoding=\"utf-8\") as file2:\n","    text1 = file1.read()\n","    text2 = file2.read()\n","\n","# Remove stop words from the input texts\n","filtered_text1 = remove_stopwords(text1)\n","filtered_text2 = remove_stopwords(text2)\n","# Define the n-gram size (e.g., 2 for bigrams, 3 for trigrams)\n","n = 1\n","\n","# Short-term\n","tokens = clean_and_tokenize(filtered_text1)\n","ngrams1 = generate_ngrams(tokens, n)\n","tokens = clean_and_tokenize(filtered_text2)\n","ngrams2 = generate_ngrams(tokens, n)\n","\n","# Compare and identify common n-grams\n","common_ngrams = compare_ngrams(filtered_text1, filtered_text2, n)\n","\n","\n","# Convert the lists to sets for efficient comparison\n","set1 = set(ngrams1)\n","set2 = set(ngrams2)\n","\n","# Find unique words in each list\n","unique_in_list1 = set1.difference(set2)\n","unique_in_list2 = set2.difference(set1)\n","\n","\n","# Display unique n-grams\n","\n","#print(\"Unique words in \"+input_file, unique_in_list1)\n","#print(\"Unique words in \"+input_file2, unique_in_list2)\n","\n","# Display the results\n","print(\"Unique words in \"+input_file+ \" saved to:\")\n","print(f\"{output_file2}\")\n","print(\"Unique words in \"+input_file2+ \" saved to:\")\n","print(f\"{output_file3}\")\n","\n","# Display common n-grams\n","print(f\"Common {n}-grams saved to:\")\n","\n","with open(output_file, \"w\") as file:\n","    for ngram in common_ngrams:\n","      #print(\" \".join(ngram))\n","      file.write(\" \".join(ngram) + \"\\n\")\n","\n","with open(output_file2, \"w\") as file:\n","    for ngram in ngrams1:\n","      #print(\" \".join(ngram))\n","      file.write(\" \".join(ngram) + \"\\n\")\n","\n","with open(output_file3, \"w\") as file:\n","    for ngram in ngrams2:\n","      #print(\" \".join(ngram))\n","      file.write(\" \".join(ngram) + \"\\n\")\n","\n","print(f\"{output_file}\")\n"],"metadata":{"id":"1cd6hBSD6zNi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706194297846,"user_tz":300,"elapsed":6784,"user":{"displayName":"SONIA YACO","userId":"08079516613956666079"}},"outputId":"53bdd5ac-8d46-4e61-ae98-6bc595c1d2a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique words in /content/drive/MyDrive/Colab Notebooks/AIGriffis/data/marg_bio_paragrapheduft8.txt saved to:\n","/content/drive/MyDrive/Colab Notebooks/AIGriffis/data/out/firstfile_ngrams.txt\n","Unique words in /content/drive/MyDrive/Colab Notebooks/AIGriffis/data/griffis_diary_by_day.txt saved to:\n","/content/drive/MyDrive/Colab Notebooks/AIGriffis/data/out/secondfile_ngrams.txt\n","Common 1-grams saved to:\n","/content/drive/MyDrive/Colab Notebooks/AIGriffis/data/out/common_ngrams.txt\n"]}]}]}