{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"eL-Ek3Smm5HS"},"outputs":[],"source":["#NER routines using Spacy and NLTK, with sorted lists and visualization\n","#Sonia Yaco\n","#Rutgers University\n","#2024"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21107,"status":"ok","timestamp":1706116645098,"user":{"displayName":"SONIA YACO","userId":"08079516613956666079"},"user_tz":300},"id":"XfPwzMO0elEX","outputId":"f2afd485-e088-4371-c356-98e2bf8a971b"},"outputs":[],"source":["#Load the Drive helper and mount\n","from google.colab import drive#\n","drive.mount('/content/drive/', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29947,"status":"ok","timestamp":1706116675038,"user":{"displayName":"SONIA YACO","userId":"08079516613956666079"},"user_tz":300},"id":"34kS5CJ3Oz-5","outputId":"6febb433-0fd7-44ff-d6ef-a9510253c4a7"},"outputs":[],"source":["#pips\n","!pip -q install spacy nltk\n","!python -m spacy download en_core_web_sm\n","!pip-q install matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17022,"status":"ok","timestamp":1706116692047,"user":{"displayName":"SONIA YACO","userId":"08079516613956666079"},"user_tz":300},"id":"PbzlBbntO2I0","outputId":"81ce5859-f302-46e9-9aea-ed88a678da15"},"outputs":[],"source":["# Import necessary libraries\n","import spacy\n","from spacy import displacy\n","from spacy.tokens import Doc, Span\n","\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.tag import pos_tag\n","from nltk.chunk import ne_chunk\n","from collections import defaultdict\n","\n","# Load the English tokenizer, tagger, parser, NER, and word vectors\n","nlp = spacy.load(\"en_core_web_sm\")\n","# Load Spacy's NER model\n","nlp_spacy = spacy.load('en_core_web_sm')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"faml7_NzO3zc"},"outputs":[],"source":["#paths and file locations\n","# modify as needed\n","text_path = \"data\"\n","# Define the path to your text files\n","input_file = text_path + \"/griffis_diary_by_day.txt\"\n","output_file = text_path + \"/output.txt\"\n","output_file2 = text_path + \"/output2.txt\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjD6ipr0O74m"},"outputs":[],"source":["#NER alpha order by word\n","# create NER with two different routines, then combine and output to text file\n","#NER with Spacy\n","def spacy_ner(text):\n","    doc = nlp_spacy(text)\n","    return [(X.text, X.label_) for X in doc.ents]\n","\n","#NER with NLTK\n","def nltk_ner(text):\n","    tokenized = word_tokenize(text)\n","    tagged = pos_tag(tokenized)\n","    entities = ne_chunk(tagged)\n","    return [(leaf[0], 'NE') for tree in entities if hasattr(tree, 'label') and tree.label() == 'NE' for leaf in tree.leaves()]\n","\n","# Read text from the input file\n","with open(input_file, 'r') as file:\n","    text = file.read()\n","\n","# Perform NER with different libraries\n","entities_spacy = spacy_ner(text)\n","entities_nltk = nltk_ner(text)\n","\n","# Combine entities from all libraries\n","combined_entities = defaultdict(list)\n","for entity, label in entities_spacy:\n","    combined_entities[label].append(entity)\n","\n","for entity, label in entities_nltk:\n","    combined_entities[label].append(entity)\n","\n","# Write recognized entities to the output file\n","with open(output_file, 'w') as file:\n","    for label, ents in combined_entities.items():\n","        file.write(f\"Label: {label}\\nEntities:\\n\")\n","        for ent in set(ents):  # Using set to avoid duplicate entities\n","            file.write(f\"{ent}\\n\")\n","        file.write(\"\\n\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wZjwYcXdV9vb"},"outputs":[],"source":["# NER in category order\n","# create NER with same first Spacy routine, but tokenized NLTK routine and output to text file\n","def spacy_ner(text):\n","    doc = nlp_spacy(text)\n","    return [(X.text, X.label_) for X in doc.ents]\n","\n","# Define a function to perform NER using NLTK\n","def nltk_ner(text):\n","    nltk_entities = ne_chunk(pos_tag(word_tokenize(text)))\n","    return [(leaf[0], 'NE' if type(leaf) is nltk.Tree else leaf[1]) for leaf in nltk_entities]\n","\n","# Function to write entities to a file\n","def write_entities_to_file(entities, file_name):\n","    with open(file_name, 'w') as f:\n","        for entity in entities:\n","            f.write(f\"{entity[0]} ({entity[1]})\\n\")\n","\n","# Read the text file\n","with open(input_file, 'r') as file:\n","    text = file.read()\n","\n","# Perform NER using SpaCy\n","spacy_entities = spacy_ner(text)\n","\n","# Perform NER using NLTK\n","nltk_entities = nltk_ner(text)\n","\n","# Write entities to output file\n","write_entities_to_file(spacy_entities, output_file)\n","\n","# Combine entities from all libraries\n","combined_entities = defaultdict(list)\n","for entity, label in spacy_entities:\n","    combined_entities[label].append(entity)\n","\n","for entity, label in nltk_entities:\n","    combined_entities[label].append(entity)\n","\n","# Write recognized entities to the output file\n","with open(output_file2, 'w') as file:\n","    for label, ents in combined_entities.items():\n","        file.write(f\"Label: {label}\\nEntities:\\n\")\n","        for ent in set(ents):  # Using set to avoid duplicate entities\n","            file.write(f\"{ent}\\n\")\n","        file.write(\"\\n\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":5062,"status":"ok","timestamp":1706116722697,"user":{"displayName":"SONIA YACO","userId":"08079516613956666079"},"user_tz":300},"id":"wocxg8-YJ1G1","outputId":"ba51fde8-365b-4543-cce1-227dd456a354"},"outputs":[],"source":["# NER color coded word visualizations\n","\n","# Load the English language model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Process the text with spaCy NLP pipeline\n","doc = nlp(text)\n","\n","#first display all NERs color coded in context\n","displacy.render(doc, style='ent', jupyter=True)\n","\n","#then display just three filtered labels, with no context\n","# Filter entities based on labels\n","filtered_entities = [ent for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'GPE']]\n","\n","# Extract tokens corresponding to filtered entities\n","filtered_tokens = [token.text for ent in filtered_entities for token in ent]\n","\n","# Create a new Doc for filtered tokens\n","filtered_doc = Doc(doc.vocab, words=filtered_tokens)\n","\n","# Adjust entities for the new Doc\n","adjusted_entities = []\n","start_offset = 0\n","for ent in filtered_entities:\n","    end_offset = start_offset + len(ent)\n","    span = Span(filtered_doc, start_offset, end_offset, label=ent.label_)\n","    adjusted_entities.append(span)\n","    start_offset = end_offset\n","\n","# Update the entities in the new Doc\n","filtered_doc.ents = adjusted_entities\n","\n","displacy.render(filtered_doc, style='ent', jupyter=True)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN6D4K4ldUV5+iZlQcgwEKu","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
